
    <html>
    <head>
    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
    <link href="style.css" rel="stylesheet">
    <title>VIEACS-TTS: AN END-TO-END VIETNAMESE MULTIPLE ACCENT TEXT-TO-SPEECH AND VOICE-CONVERSION MODEL FOR EDGE DEVICE</title>
    </head>
    <body>

        <div id="header" class="container-fluid">
            <div class="row" style="text-align: center;"/>
            <!--<div class="logoimg">
                <img src="meta_logo.png" height="95">
                <img src="huji_logo.png" height="95">
            </div>-->
                <h1>VIEACS-TTS: An end-to-end Vietnamese multipe accent Text-to-speech and Voice-conversion model for edge device</h1>
                                <div class="authors">
                    <a href="">Hung Vo</a><sup>1</sup>,
                </div>

                <br>
                <p><sup>1</sup>Ho Chi Minh University of Technology (HCMUT) <sup>

                 <div class="assets">
                    <a href="paper.pdf" target="_blank">[paper]</a><a href="https://github.com/Text-to-Audio/Make-An-Audio" target="_blank">[github]</a>
                </div>
                
        </div>

    <div class="container">
        <h2>Abstract</h2>
        <span>
            Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind due to two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach which alleviates the data scarcity by using weekly-supervised data with language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective evaluation. Moreover, we present its controllability with classifier-free guidance and generalization for X-to-Audio with "No Modality Left Behind", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input.
        </span>
    </div>

    
      

    </body>
    </html>
    
